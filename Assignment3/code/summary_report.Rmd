---
title: "Fast Growth Firms Prediction Model"
author: "Nawal Zehra Hasan"
date: "`r Sys.Date()`"
output:
  html_document:
    highlighter: null
    theme: flatly
    toc: yes
    toc_float: yes
    fig_caption: yes
  pdf_document:
    toc: yes
params:
  dynamictitle: Firm Fast Growth Prediction
  viridis_palette: viridis
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=F}
#### SET UP
rm(list=ls())
# Import libraries
library(haven)
library(glmnet)
library(purrr)
library(margins)
library(skimr)
library(kableExtra)
library(Hmisc)
library(cowplot)
library(gmodels) 
library(lspline)
library(sandwich)
library(modelsummary)
library(rattle)
library(caret)
library(pROC)
library(ranger)
library(rpart)
library(partykit)
library(rpart.plot)
library(viridis)

# Please change path to yours
path <- "/Users/nawalhasan/Desktop/Assignment3/"
# load functions
source("https://raw.githubusercontent.com/nawalhasan/DA3/main/Assignment3/code/helper_functions.R")
source("https://raw.githubusercontent.com/nawalhasan/DA3/main/Assignment3/code/theme_bg.R")

data_in <- paste0(path,"data/clean/")
data_out <- data_in
output <- paste0(path,"output/")
```


```{r, include=F}
# Load the data
data <- readRDS(paste0(data_in,"bisnode_firms_clean.rds"))
# Define variable sets -----------------------------------------------------------------------
rawvars <-  c("curr_assets", "curr_liab", "extra_exp", "extra_inc", "extra_profit_loss", "fixed_assets",
              "inc_bef_tax", "intang_assets", "inventories", "liq_assets", "material_exp", "personnel_exp",
              "profit_loss_year", "sales", "share_eq", "subscribed_cap")
engvar <- c("total_assets_bs", "fixed_assets_bs", "liq_assets_bs", "curr_assets_bs",
            "share_eq_bs", "subscribed_cap_bs", "intang_assets_bs", "extra_exp_pl",
            "extra_inc_pl", "extra_profit_loss_pl", "inc_bef_tax_pl", "inventories_pl",
            "material_exp_pl", "profit_loss_year_pl", "personnel_exp_pl")
engvar2 <- c("extra_profit_loss_pl_quad", "inc_bef_tax_pl_quad",
             "profit_loss_year_pl_quad", "share_eq_bs_quad")
engvar3 <- c(grep("*flag_low$", names(data), value = TRUE),
             grep("*flag_high$", names(data), value = TRUE),
             grep("*flag_error$", names(data), value = TRUE),
             grep("*flag_zero$", names(data), value = TRUE))
d1 <-  c("d1_sales_mil_log_mod", "d1_sales_mil_log_mod_sq",
         "flag_low_d1_sales_mil_log", "flag_high_d1_sales_mil_log")
hr <- c("female", "ceo_age", "flag_high_ceo_age", "flag_low_ceo_age",
        "flag_miss_ceo_age", "ceo_count", "labor_avg_mod",
        "flag_miss_labor_avg", "foreign_management")
firm <- c("age", "age2", "new", "ind2_cat", "m_region_loc", "urban_m")
# interactions for logit, LASSO
interactions1 <- c("ind2_cat*age", "ind2_cat*age2",
                   "ind2_cat*d1_sales_mil_log_mod", "ind2_cat*sales_mil_log",
                   "ind2_cat*ceo_age", "ind2_cat*foreign_management",
                   "ind2_cat*female",   "ind2_cat*urban_m", "ind2_cat*labor_avg_mod")
interactions2 <- c("sales_mil_log*age", "sales_mil_log*female",
                   "sales_mil_log*profit_loss_year_pl", "sales_mil_log*foreign_management")
X1 <- c("sales_mil_log", "sales_mil_log_sq", "d1_sales_mil_log_mod", "profit_loss_year_pl", "ind2_cat")
X2 <- c("sales_mil_log", "sales_mil_log_sq", "d1_sales_mil_log_mod", "profit_loss_year_pl", "ind2_cat", "fixed_assets_bs","share_eq_bs","curr_liab_bs ",   "curr_liab_bs_flag_high ", "curr_liab_bs_flag_error",  "age","foreign_management")
X3 <- c("sales_mil_log", "sales_mil_log_sq", firm, engvar, d1)
X4 <- c("sales_mil_log", "sales_mil_log_sq", firm, engvar, engvar2, engvar3, d1, hr)
X5 <- c("sales_mil_log", "sales_mil_log_sq", firm, engvar, engvar2, engvar3, d1, hr, interactions1, interactions2)
# for LASSO
logitvars <- c("sales_mil_log", "sales_mil_log_sq", engvar, engvar2, engvar3, d1, hr, firm, interactions1, interactions2)
# for RF (no interactions, no modified features)
rfvars  <-  c("sales_mil", "d1_sales_mil_log", rawvars, hr, firm)
```

## EXECUTIVE SUMMARY

An investment company is seeking for potential investment opportunities in firms that have growth potential of at least 30% annually. For this they used prediction analysis using data compiled by  Bisnode, a data and analytics company (www.bisnode.com). To achieve this they ran several prediction models including Logit, Lasso, Random Forest to assess the results of each and select the most suitable model giving the lowest average RMSE and highest average AUC. 

I constructed prediction models that predict a firms success. Several features of the firms were taken into consideration including balance sheets, profit and loss statements, assets, expenditures and age of employees. We compared models including LASS0, Random Forest and OLS logit model that used log sales, with firm variables, human resource characteristics. Overall *16%* of the firms have a fast growth in a random sample. With the best model for predicting growth the RMSE was *0.352*, the AUC was *0.706* and the average expected loss was *0.409* with 33 predictor variables. However, I chose logit model 3 as the best model as it came out to be the best model with logit and lasso. 

From the five models with logit probability, with increasing complexity and number of predictors, model 3 was the best with an RMSE of 0.35 and AUC of 0.69. Comparing that to lasso, the RMSE for lasso was slightly lower but the AUC was also lower for lasso. I chose logit model 3. Next, with random forest comparison with our chosen model 3, the RMSE was a little lower and AUC was higher. I chose the random forest model to draw the roc curve.As threshold values lower the rate of True positives increases but it also simultaneously leads to a higher False positive rate. There is a solution to this; the loss function.

The loss function with a classification threshold we can turn predicted probabilities into classifications. With this we can look for an optimal threshold in each of our models. Finally, we can decide the best model for prediction according to the lowest average expected loss.

I defined the false negative as being three times more worse than false positive as the company will be missing on investment opportunities in a firm that is actually fast growing but was deemed as not fast growing. Thus, our loss function which shows the cost of a false negative error is thrice more than the cost of false positive. Given that, we can calculate a threshold where we can minimize this cost. Hence, our optimal threshold is **0.293**. The accuracy of our model is 80%, sensitivity is 30%, specificity is 91% and correctly predicted positives is 38%. 

Link to project [Github](https://github.com/nawalhasan/DA3/tree/main/Assignment3) repository.

## DATA CLEANING

The data initially had 287 829 observations on 46,000 firms from 2005 to 2016 with 48 variables. In the end of data cleaning I ended up with 11 910 observations and 116 variables explaining all available information like properties of company, balance sheet, profit and loss elements and management information. Hence I chose that if a company with 30% Compound Annual Growth Rate (CAGR) annual sales growth with be a fast growing firm. 

I filtered the data for the 2011-2014 period, and included firms that made a revenue between 1000 - 10 million Euros. I dropped observations with firms that are not active anymore. I also chose. I chose 2011-2012 as two years to see change in sales. I also added explanatory variables such as log of sales, ceo's age, flagged variables. A dummy variable was created as fast growth for firms with CAGR growth being greater than 30%. The distribution of our key variable, CAGR growth, can be seen below. I included ratios for key financials by dividing all balance sheet elements with total assets, and with profit and loss statement elements.

```{r  message=FALSE, warning=FALSE, echo=FALSE, fig.align="center",out.width = '50%', fig.height=4}
# Distribution of CAGR growth
ggplot(data=data, aes(x=cagr_sales)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 10, boundary=0,
                 color = "black", fill = "#1c9099") +
  coord_cartesian(xlim = c(-100, 200)) +
  labs(title="Distribution of CAGR growth (2011-2012)", x = "CAGR growth in percentage",y = "Percent")+
  theme_bw() 
```

## MODEL SELECTION

I selected the best model for predicting fast growth based on expected losses incurred using each model. As we saw above that random forest model incurred the lowest expect loss followed by logit model 3 and then LASSO. If we want to evaluate using AUC or RMSE, the results will be the same order. After seeing the results, I would still go with logit model 3 which is X3 as my final model. This is because it came out to be the best model for lasso and logit both. Secondly, I believe the simpler the model the more interpretable and solid our results are.

```{r, echo=F, warning=F, message=F, include=FALSE}
set.seed(2022)
train_indices <- as.integer(createDataPartition(data$fast_growth, p = 0.8, list = FALSE))
data_train <- data[train_indices, ]
data_holdout <- data[-train_indices, ]
data %>% 
  group_by(fast_growth_f) %>% 
  summarise("Number of companies" = n(), "Percentage" = paste0(round(n()/11911*100),'%')) %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
  
# 5 fold cross-validation ----------------------------------------------------------------------
train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)
```

```{r, echo=F, message=F, warning=F, include=FALSE}
models <- data.frame(row.names = c("X1 model", "X2 model", "X3 model", "X4 model", "X5 model"))
models$Variables[1] <- "Log sales + Log sales^2 + Change in Sales + Profit and loss + Industry"
models$Variables[2] <- "X1 + Fixed assets + Equity + Current liabilities (and flags) + Age + Foreign management"
models$Variables[3] <- "Log sales + Log sales^2 + Firm + Engine variables 1 + D1"
models$Variables[4] <- "X3 + Engine variables 2 + Engine variables 3 + HR"
models$Variables[5] <- "X4 + Interactions 1 and 2"
models %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r, include=F}
# Logit Models ----------------------------------------------
logit_model_vars <- list("X1" = X1, "X2" = X2, "X3" = X3, "X4" = X4, "X5" = X5)
CV_RMSE_folds <- list()
logit_models <- list()
for (model_name in names(logit_model_vars)) {
  features <- logit_model_vars[[model_name]]
  set.seed(2022)
  glm_model <- train(
    formula(paste0("fast_growth_f ~", paste0(features, collapse = " + "))),
    method = "glm",
    data = data_train,
    family = binomial,
    trControl = train_control
  )
  logit_models[[model_name]] <- glm_model
  # Calculate RMSE on test for each fold
  CV_RMSE_folds[[model_name]] <- glm_model$resample[,c("Resample", "RMSE")]}

# LASSO ---------------------------------------------------------
lambda <- 10^seq(-1, -4, length = 10)
grid <- expand.grid("alpha" = 1, lambda = lambda)
set.seed(2022)
system.time({
  logit_lasso_model <- train(
    formula(paste0("fast_growth_f ~", paste0(logitvars, collapse = " + "))),
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    family = "binomial",
    trControl = train_control,
    tuneGrid = grid,
    na.action=na.exclude
  )
})
tuned_logit_lasso_model <- logit_lasso_model$finalModel
best_lambda <- logit_lasso_model$bestTune$lambda
logit_models[["LASSO"]] <- logit_lasso_model
lasso_coeffs <- as.matrix(coef(tuned_logit_lasso_model, best_lambda))
write.csv(lasso_coeffs, paste0(output, "lasso_logit_coeffs.csv"))
CV_RMSE_folds[["LASSO"]] <- logit_lasso_model$resample[,c("Resample", "RMSE")]
```

```{r,  echo=F, message=F, warning=F, include=FALSE}
# Draw ROC Curve and calculate AUC for each folds --------------------------------
CV_AUC_folds <- list()
for (model_name in names(logit_models)) {
  auc <- list()
  model <- logit_models[[model_name]]
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    roc_obj <- roc(cv_fold$obs, cv_fold$fast_growth)
    auc[[fold]] <- as.numeric(roc_obj$auc)
  }
  CV_AUC_folds[[model_name]] <- data.frame("Resample" = names(auc),
                                              "AUC" = unlist(auc))
}
# For each model: average RMSE and average AUC for models ----------------------------------
CV_RMSE <- list()
CV_AUC <- list()
for (model_name in names(logit_models)) {
  CV_RMSE[[model_name]] <- mean(CV_RMSE_folds[[model_name]]$RMSE)
  CV_AUC[[model_name]] <- mean(CV_AUC_folds[[model_name]]$AUC)
}
# We have 6 models, (5 logit and the logit lasso). For each we have a 5-CV RMSE and AUC.
# We pick our preferred model based on that. -----------------------------------------------
nvars <- lapply(logit_models, FUN = function(x) length(x$coefnames))
nvars[["LASSO"]] <- sum(lasso_coeffs != 0)
logit_summary1 <- data.frame("Number of predictors" = unlist(nvars),
                             "CV RMSE" = unlist(CV_RMSE),
                             "CV AUC" = unlist(CV_AUC))
logit_summary1 %>% 
  slice(1:5) %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")

```

```{r, echo=F, message=F, warning=F, include=FALSE}
logit_summary1 %>% 
  slice(c(3,6)) %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r, echo=F, message=F, warning=F, include=FALSE, include=FALSE}
# 5 fold cross-validation
train_control <- trainControl(
  method = "cv",
  n = 5,
  classProbs = TRUE, # same as probability = TRUE in ranger
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)
train_control$verboseIter <- TRUE

tune_grid <- expand.grid(
  .mtry = c(5, 6, 7),
  .splitrule = "gini",
  .min.node.size = c(10, 15)
)

# build rf model
set.seed(2022)
rf_model_p <- train(
  formula(paste0("fast_growth_f ~ ", paste0(rfvars , collapse = " + "))),
  method = "ranger",
  data = data_train,
  tuneGrid = tune_grid,
  trControl = train_control,
  importance = "impurity"
)

rf_model_p$results

saveRDS(rf_model_p, paste0(data_out, "rf_model_p.rds"))

best_mtry <- rf_model_p$bestTune$mtry
best_min_node_size <- rf_model_p$bestTune$min.node.size

CV_RMSE_folds[["rf_p"]] <- rf_model_p$resample[,c("Resample", "RMSE")]
```

```{r, echo=F, message=F, warning=F, include=FALSE}
# Get average (ie over the folds) RMSE and AUC ------------------------------
auc <- list()
for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    rf_model_p$pred %>%
    filter(Resample == fold)
  
  roc_obj <- roc(cv_fold$obs, cv_fold$fast_growth)
  auc[[fold]] <- as.numeric(roc_obj$auc)
}
CV_AUC_folds[["rf_p"]] <- data.frame("Resample" = names(auc),
                                     "AUC" = unlist(auc))
CV_RMSE[["Random_forest"]] <- mean(CV_RMSE_folds[["rf_p"]]$RMSE)
CV_AUC[["Random_forest"]] <- mean(CV_AUC_folds[["rf_p"]]$AUC)
rf_summary <- data.frame("CV RMSE" = unlist(CV_RMSE),
                         "CV AUC" = unlist(CV_AUC))
```

```{r, echo=F, message=F, warning=F, include=FALSE}
rf_summary %>% 
  slice(c(3,7)) %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```


```{r, echo=F, message=F, warning=F, out.width="50%", include=FALSE}
best_no_loss <- rf_model_p
predicted_probabilities_holdout <- predict(best_no_loss, newdata = data_holdout, type = "prob")
data_holdout[,"best_no_loss_pred"] <- predicted_probabilities_holdout[,"fast_growth"]
# discrete ROC (with thresholds in steps) on holdout -------------------------------------------------
thresholds <- seq(0.05, 0.75, by = 0.025)
cm <- list()
true_positive_rates <- c()
false_positive_rates <- c()
for (thr in thresholds) {
  holdout_prediction <- ifelse(data_holdout[,"best_no_loss_pred"] < thr, "no_fast_growth", "fast_growth") %>%
    factor(levels = c("no_fast_growth", "fast_growth"))
  cm_thr <- confusionMatrix(holdout_prediction,data_holdout$fast_growth_f)$table
  cm[[as.character(thr)]] <- cm_thr
  true_positive_rates <- c(true_positive_rates, cm_thr["fast_growth", "fast_growth"] /
                             (cm_thr["fast_growth", "fast_growth"] + cm_thr["no_fast_growth", "fast_growth"]))
  false_positive_rates <- c(false_positive_rates, cm_thr["fast_growth", "no_fast_growth"] /
                              (cm_thr["fast_growth", "no_fast_growth"] + cm_thr["no_fast_growth", "no_fast_growth"]))
}
tpr_fpr_for_thresholds <- tibble(
  "threshold" = thresholds,
  "true_positive_rate" = true_positive_rates,
  "false_positive_rate" = false_positive_rates
)
ggplot(
  data = tpr_fpr_for_thresholds,
  aes(x = false_positive_rate, y = true_positive_rate, color = threshold)) +
  labs(x = "False positive rate (1 - Specificity)", y = "True positive rate (Sensitivity)") +
  geom_point(size=2, alpha=0.8) +
  scale_color_viridis(option = "D", direction = -1) +
  scale_x_continuous(expand = c(0.01,0.01), limit=c(0,1), breaks = seq(0,1,0.1)) +
  scale_y_continuous(expand = c(0.01,0.01), limit=c(0,1), breaks = seq(0,1,0.1)) +
  theme_bw() +
  theme(legend.position ="right") +
  theme(legend.title = element_text(size = 4), 
        legend.text = element_text(size = 4),
        legend.key.size = unit(.4, "cm")) 
# continuous ROC on holdout with best model (Logit 4) -------------------------------------------
roc_obj_holdout <- roc(data_holdout$fast_growth, data_holdout$best_no_loss_pred)
createRocPlot(roc_obj_holdout, "best_no_loss_roc_plot_holdout")
```

```{r, echo=F, message=F, warning=F}
FP=1
FN=3
cost = FN/FP
# the prevalence, or the proportion of cases in the population (n.cases/(n.controls+n.cases))
prevelance = sum(data_train$fast_growth)/length(data_train$fast_growth)
# LOGIT AND LASSO ------------------------------------------------------------------------------
# Draw ROC Curve and find optimal threshold with loss function --------------------------
best_tresholds <- list()
expected_loss <- list()
logit_cv_rocs <- list()
logit_cv_threshold <- list()
logit_cv_expected_loss <- list()
for (model_name in names(logit_models)) {
  
  model <- logit_models[[model_name]]
  colname <- paste0(model_name,"_prediction")
  
  best_tresholds_cv <- list()
  expected_loss_cv <- list()
  
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    
    roc_obj <- roc(cv_fold$obs, cv_fold$fast_growth)
    best_treshold <- coords(roc_obj, "best", ret="all", transpose = FALSE,
                            best.method="youden", best.weights=c(cost, prevelance))
    best_tresholds_cv[[fold]] <- best_treshold$threshold
    expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$fast_growth)
  }
  
  # average
  best_tresholds[[model_name]] <- mean(unlist(best_tresholds_cv))
  expected_loss[[model_name]] <- mean(unlist(expected_loss_cv))
  
  # for fold #5
  logit_cv_rocs[[model_name]] <- roc_obj
  logit_cv_threshold[[model_name]] <- best_treshold
  logit_cv_expected_loss[[model_name]] <- expected_loss_cv[[fold]]
  
}
# RANDOM FOREST --------------------------------------------------------
# Now use loss function and search for best thresholds and expected loss over folds -----
best_tresholds_cv <- list()
expected_loss_cv <- list()
for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    rf_model_p$pred %>%
    filter(mtry == best_mtry,
           min.node.size == best_min_node_size,
           Resample == fold)
  
  roc_obj <- roc(cv_fold$obs, cv_fold$fast_growth)
  best_treshold <- coords(roc_obj, "best", ret="all", transpose = FALSE,
                          best.method="youden", best.weights=c(cost, prevelance))
  best_tresholds_cv[[fold]] <- best_treshold$threshold
  expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$fast_growth)
}
# average
best_tresholds[["rf_p"]] <- mean(unlist(best_tresholds_cv))
expected_loss[["rf_p"]] <- mean(unlist(expected_loss_cv))
# Save output --------------------------------------------------------
# Model selection is carried out on this CV RMSE
nvars[["rf_p"]] <- length(rfvars)
summary_results <- data.frame("Number of predictors" = unlist(nvars),
                              "CV RMSE" = unlist(CV_RMSE),
                              "CV AUC" = unlist(CV_AUC),
                              "CV threshold" = unlist(best_tresholds),
                              "CV expected Loss" = unlist(expected_loss))
model_names <- c("Logit X1", "Logit X3",
                 "LASSO","RF probability")

summary_results <- summary_results %>%
  filter(rownames(.) %in% c("X1", "X3", "LASSO", "rf_p"))
rownames(summary_results) <- model_names
summary_results %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```
## THE LOSS FUNCTION

Thus, our loss function which shows the cost of a false negative error is thrice more than the cost of false positive. Given that, we can calculate a threshold where we can minimize this cost. The formula for optimal classification threshold returns with **0.25=(1/(1+3))**. This is close to the selected model in the table below. The table below shows the results of a optimal threshold selection algorithm that we ran on the train set with a 5-fold cross validation. The lowest RMSE and the lowest expected loss belong to the same model which is random forest. Hence, our optimal threshold is **0.28**.

### CONFUSION MATRIX

Now that we have decided on our optimal threshold, we can use our holdout set that we had earlier separated(20% of our observations) and evaluate our best model which is Logit X3. This is the final step part of classification.

The expected loss calculated on the holdout set using logit model X3 is **0.39**. This is lower than the expected loss on the train set. Below we have the confusion matrix from which we can calculate the true positive, true negative, false positive and false negative predictions made by our model.

```{r, echo=F, message=F, warning=F}
best_logit_with_loss <- logit_models[["X3"]]
best_logit_optimal_treshold <- best_tresholds[["X3"]]
logit_predicted_probabilities_holdout <- predict(best_logit_with_loss, newdata = data_holdout, type = "prob")
data_holdout[,"best_logit_with_loss_pred"] <- logit_predicted_probabilities_holdout[,"fast_growth"]
# ROC curve on holdout
roc_obj_holdout <- roc(data_holdout$fast_growth, data_holdout[, "best_logit_with_loss_pred", drop=TRUE])
# Get expected loss on holdout
holdout_treshold <- coords(roc_obj_holdout, x = best_logit_optimal_treshold, input= "threshold",
                           ret="all", transpose = FALSE)
expected_loss_holdout <- (holdout_treshold$fp*FP + holdout_treshold$fn*FN)/length(data_holdout$fast_growth)
# expected loss: 0.393

# Confusion table on holdout with optimal threshold
holdout_prediction <-
  ifelse(data_holdout$best_logit_with_loss_pred < best_logit_optimal_treshold, "no_fast_growth", "fast_growth") %>%
  factor(levels = c("no_fast_growth", "fast_growth"))
cm_object3 <- confusionMatrix(holdout_prediction,data_holdout$fast_growth_f)
cm3 <- cm_object3$table
cm3 %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```
## CONCLUSION

Our analysis intended to find out whether a company can have 30% growth in their sales in one year time. My choice of model is logit model 3 with financial variables, firm characteristics, log sales. Among the firms that the model predicted to be fast growing 39% actually turned out to be fast growing. This is much higher than the chances a firm being fast growing in a random sample which was 16%.
If a company intends to use our chosen model then it is important that the loss function is very well defined according to the business question at hand. To allow for external validity this model should be applied to a wider time period i.e. every year between 2005 and 2016. We can then see whether the coefficients remain important throughout the decade.

